{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10409147,"sourceType":"datasetVersion","datasetId":6450551},{"sourceId":10549491,"sourceType":"datasetVersion","datasetId":6527299},{"sourceId":12431403,"sourceType":"datasetVersion","datasetId":7841431},{"sourceId":12431421,"sourceType":"datasetVersion","datasetId":7841445},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":3900,"modelId":1902}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adityabhaskar12/rag-originbluy?scriptVersionId=249815798\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install opencv-python\n!pip install PyMuPDF\n!pip install transformers\n!pip install torch\n!pip install numpy\n!pip install sentence-transformers\n!pip install pdf2image\n!pip install python-docx\n!pip install mammoth\n!pip install chromadb\n!pip install Pillow\n!pip install git+https://github.com/JaidedAI/EasyOCR.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:48:05.760643Z","iopub.execute_input":"2025-07-10T12:48:05.761293Z","iopub.status.idle":"2025-07-10T12:50:40.252379Z","shell.execute_reply.started":"2025-07-10T12:48:05.761268Z","shell.execute_reply":"2025-07-10T12:50:40.25162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt update\n!sudo apt install -y poppler-utils tesseract-ocr libgl1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:50:40.253972Z","iopub.execute_input":"2025-07-10T12:50:40.254222Z","iopub.status.idle":"2025-07-10T12:50:50.941099Z","shell.execute_reply.started":"2025-07-10T12:50:40.254197Z","shell.execute_reply":"2025-07-10T12:50:50.94023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:50:50.942218Z","iopub.execute_input":"2025-07-10T12:50:50.943027Z","iopub.status.idle":"2025-07-10T12:50:56.735283Z","shell.execute_reply.started":"2025-07-10T12:50:50.942998Z","shell.execute_reply":"2025-07-10T12:50:56.734176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U langchain-huggingface transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:50:56.73723Z","iopub.execute_input":"2025-07-10T12:50:56.737478Z","iopub.status.idle":"2025-07-10T12:51:09.017462Z","shell.execute_reply.started":"2025-07-10T12:50:56.737452Z","shell.execute_reply":"2025-07-10T12:51:09.016402Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport cv2\nimport fitz \nfrom docx import Document\nfrom typing import List, Dict, Any, Optional\nfrom pdf2image import convert_from_path\nimport easyocr\nimport torch\nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document as LangchainDocument\nfrom langchain.prompts import PromptTemplate\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    pipeline,\n    TableTransformerForObjectDetection,\n    DetrImageProcessor\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:51:09.018795Z","iopub.execute_input":"2025-07-10T12:51:09.019427Z","iopub.status.idle":"2025-07-10T12:51:35.536634Z","shell.execute_reply.started":"2025-07-10T12:51:09.019389Z","shell.execute_reply":"2025-07-10T12:51:35.536051Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"class Config:\n    CHROMA_DIR = \"chroma_db\"\n    MISTRAL_PATH = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n    SUPPORTED_EXTS = {'.pdf', '.docx', '.txt'}\n    MAX_DOC_SIZE = 100 * 1024 * 1024  \n    MAX_DOCUMENTS = 10\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:51:35.541739Z","iopub.execute_input":"2025-07-10T12:51:35.542032Z","iopub.status.idle":"2025-07-10T12:51:35.55561Z","shell.execute_reply.started":"2025-07-10T12:51:35.542009Z","shell.execute_reply":"2025-07-10T12:51:35.555111Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"class DocumentProcessor:\n    def __init__(self):\n        try:\n            self.ocr_reader = easyocr.Reader(['en'])\n            self.table_processor = DetrImageProcessor.from_pretrained(\n               \"microsoft/table-transformer-detection\",\n                size={\"longest_edge\": 1000}, \n                use_fast=True\n            )\n            self.table_detector = TableTransformerForObjectDetection.from_pretrained(\n                \"microsoft/table-transformer-detection\"\n            ).to(Config.DEVICE)\n        except Exception as e:\n            print(f\"Initialization error: {str(e)}\")\n            raise\n\n    def process_file(self, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Main method to process any supported file type\"\"\"\n        ext = os.path.splitext(file_path)[1].lower()\n        try:\n            if ext == '.pdf':\n                return self._process_pdf(file_path)\n            elif ext == '.docx':\n                return self._process_docx(file_path)\n            elif ext == '.txt':\n                return self._process_txt(file_path)\n        except Exception as e:\n            print(f\"Error processing {file_path}: {str(e)}\")\n            return []\n\n    def _process_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Process PDF with OCR and table detection, with robust fallback\"\"\"\n        try:\n            advanced_result = self._process_pdf_with_ocr_and_tables(pdf_path)\n            if advanced_result:\n                return advanced_result\n        except Exception as e:\n            print(f\"Advanced processing failed: {str(e)}\")\n        try:\n            doc = fitz.open(pdf_path)\n            return [{\n                \"page_number\": i + 1,\n                \"content\": [{\"type\": \"text\", \"content\": page.get_text()}],\n                \"source\": os.path.basename(pdf_path)\n            } for i, page in enumerate(doc)]\n        except Exception as e:\n            print(f\"Simple PDF processing failed: {str(e)}\")\n            return []\n\n    def _process_pdf_with_ocr_and_tables(self, pdf_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Advanced PDF processing with OCR and table detection\"\"\"\n        try:\n            images = convert_from_path(pdf_path)\n            doc = fitz.open(pdf_path)\n            extracted_data = []\n\n            for page_num, (image, page) in enumerate(zip(images, doc), 1):\n                page_content = []\n                cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n\n                # TABLE DETECTION (only if GPU available)\n                table_boxes = []\n                if torch.cuda.is_available():\n                    try:\n                        inputs = self.table_processor(images=image, return_tensors=\"pt\").to(Config.DEVICE)\n                        with torch.no_grad():\n                            outputs = self.table_detector(**inputs)\n\n                        target_sizes = torch.tensor([image.size[::-1]])\n                        results = self.table_processor.post_process_object_detection(\n                            outputs, threshold=0.9, target_sizes=target_sizes\n                        )[0]\n\n                        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n                            if score > 0.9 and label == self.table_detector.config.id2label[1]:\n                                box = [int(i) for i in box.tolist()]\n                                table_boxes.append((box[1], box[0], box[3], box[2]))\n                    except Exception as e:\n                        print(f\"Table detection failed, continuing without tables: {str(e)}\")\n                try:\n                    if table_boxes:\n                        mask = np.ones(cv_image.shape[:2], dtype=np.uint8) * 255\n                        for y0, x0, y1, x1 in table_boxes:\n                            cv2.rectangle(mask, (x0, y0), (x1, y1), 0, -1)\n                        masked_image = cv2.bitwise_and(cv_image, cv_image, mask=mask)\n                    else:\n                        masked_image = cv_image\n                    ocr_results = self.ocr_reader.readtext(masked_image, paragraph=True)\n                    for result in ocr_results:\n                        try:\n                            if len(result) >= 2:\n                                text = result[1]\n                                bbox = result[0]\n                                page_content.append({\n                                    \"type\": \"text\",\n                                    \"content\": text,\n                                    \"position\": [int(coord) for point in bbox for coord in point] if bbox else []\n                                })\n                        except Exception as e:\n                            print(f\"Error processing OCR result: {str(e)}\")\n\n                except Exception as e:\n                    print(f\"OCR failed for page {page_num}, falling back to simple text: {str(e)}\")\n                    page_content.append({\n                        \"type\": \"text\",\n                        \"content\": page.get_text(),\n                        \"position\": []\n                    })\n                for y0, x0, y1, x1 in table_boxes:\n                    try:\n                        table_img = cv_image[y0:y1, x0:x1]\n                        table_results = self.ocr_reader.readtext(table_img)\n                        table_content = [res[1] for res in table_results if len(res) >= 2]\n\n                        page_content.append({\n                            \"type\": \"table\",\n                            \"content\": table_content,\n                            \"position\": [x0, y0, x1, y1]\n                        })\n                    except Exception as e:\n                        print(f\"Table processing failed: {str(e)}\")\n\n                extracted_data.append({\n                    \"page_number\": page_num,\n                    \"content\": page_content,\n                    \"source\": os.path.basename(pdf_path)\n                })\n\n            return extracted_data\n\n        except Exception as e:\n            raise Exception(f\"PDF processing failed: {str(e)}\")\n\n    def _process_docx(self, docx_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Process DOCX files with table support\"\"\"\n        try:\n            doc = Document(docx_path)\n            content = []\n            for para in doc.paragraphs:\n                if para.text.strip():\n                    content.append({\n                        \"type\": \"text\",\n                        \"content\": para.text\n                    })\n            for table in doc.tables:\n                table_data = []\n                for row in table.rows:\n                    row_data = [cell.text for cell in row.cells]\n                    table_data.append(row_data)\n                content.append({\n                    \"type\": \"table\",\n                    \"content\": table_data\n                })\n\n            return [{\n                \"page_number\": 1,\n                \"content\": content,\n                \"source\": os.path.basename(docx_path)\n            }]\n        except Exception as e:\n            print(f\"DOCX processing error: {str(e)}\")\n            return []\n\n    def _process_txt(self, txt_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Process plain text files\"\"\"\n        try:\n            with open(txt_path, 'r', encoding='utf-8') as f:\n                return [{\n                    \"page_number\": 1,\n                    \"content\": [{\"type\": \"text\", \"content\": f.read()}],\n                    \"source\": os.path.basename(txt_path)\n                }]\n        except Exception as e:\n            print(f\"TXT processing error: {str(e)}\")\n            return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:51:35.556337Z","iopub.execute_input":"2025-07-10T12:51:35.556604Z","iopub.status.idle":"2025-07-10T12:51:35.578309Z","shell.execute_reply.started":"2025-07-10T12:51:35.556578Z","shell.execute_reply":"2025-07-10T12:51:35.577705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"class VectorStoreManager:\n    def __init__(self):\n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=Config.EMBEDDING_MODEL,\n            model_kwargs={'device': Config.DEVICE}\n        )\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200\n        )\n\n    def create_store(self, documents: List[Dict]) -> Optional[Chroma]:\n        if not documents:\n            return None\n\n        lc_docs = []\n        for doc in documents:\n            for content in doc['content']:\n                if content['type'] == 'text':\n                    for chunk in self.text_splitter.split_text(content['content']):\n                        lc_docs.append(LangchainDocument(\n                            page_content=chunk,\n                            metadata={} \n                        ))\n                elif content['type'] == 'table':\n                    table_text = \" | \".join([\" | \".join(row) for row in content['content']])\n                    lc_docs.append(LangchainDocument(\n                        page_content=f\"TABLE: {table_text}\",\n                        metadata={}\n                    ))\n\n        return Chroma.from_documents(\n            documents=lc_docs,\n            embedding=self.embedding_model,\n            persist_directory=Config.CHROMA_DIR\n        ) if lc_docs else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:51:35.579056Z","iopub.execute_input":"2025-07-10T12:51:35.579392Z","iopub.status.idle":"2025-07-10T12:51:35.590643Z","shell.execute_reply.started":"2025-07-10T12:51:35.579368Z","shell.execute_reply":"2025-07-10T12:51:35.590063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MistralLLM:\n    def __init__(self):\n        try:\n            from langchain_community.llms import HuggingFacePipeline  # Correct import\n            \n            self.tokenizer = AutoTokenizer.from_pretrained(Config.MISTRAL_PATH)\n            self.model = AutoModelForCausalLM.from_pretrained(\n                Config.MISTRAL_PATH,\n                torch_dtype=torch.float16,\n                device_map=\"auto\"\n            )\n            self.pipe = pipeline(\n                \"text-generation\",\n                model=self.model,\n                tokenizer=self.tokenizer,\n                max_new_tokens=512,\n                temperature=0.1,\n                device_map=\"auto\"\n            )\n            self.llm = HuggingFacePipeline(pipeline=self.pipe)\n        except Exception as e:\n            print(f\"LLM initialization failed: {str(e)}\")\n            self.llm = None\n\n    def query(self, vectordb: Chroma, query: str, k: int = 3) -> str:\n        if not self.llm:\n            return \"LLM not initialized\"\n            \n        retriever = vectordb.as_retriever(search_kwargs={\"k\": k})\n        qa = RetrievalQA.from_chain_type(\n            llm=self.llm,\n            chain_type=\"stuff\",\n            retriever=retriever,\n            return_source_documents=True\n        )\n        result = qa({\"query\": query})\n        return f\"Answer: {result['result']}\\nSources: {[d.metadata['source'] for d in result['source_documents']]}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:51:35.592778Z","iopub.execute_input":"2025-07-10T12:51:35.593478Z","iopub.status.idle":"2025-07-10T12:51:35.605826Z","shell.execute_reply.started":"2025-07-10T12:51:35.593455Z","shell.execute_reply":"2025-07-10T12:51:35.605182Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"RAG_PROMPT_TEMPLATE = \"\"\"As an expert document analyst with access to multiple sources, your task is to provide the most accurate, well-structured answer to the user's question:\n\n1. Context Analysis:\n{context}\n\n2. Question:\n{question}\n\n3. Answer Generation Rules:\n- Synthesize information from all relevant context\n- Acknowledge when information is incomplete\n- Break down complex answers into bullet points when helpful\n- Highlight key statistics, names, and dates\n- If conflicting information exists, present both sides with sources\n- For technical queries, provide detailed explanations\n- For summary requests, include all major points\n\n4. Required Answer Format:\n[Start of Answer]\n### Comprehensive Response:\n[Your synthesized answer here]\nonly include relevant sources\n[End of Answer]\"\"\"\nrag_prompt = PromptTemplate(\n    template=RAG_PROMPT_TEMPLATE,\n    input_variables=[\"context\", \"question\"]\n)\ndef create_qa_chain(llm, vectordb, k=5):\n    return RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=vectordb.as_retriever(search_kwargs={\"k\": k}),\n        return_source_documents=False,  # Disable source tracking\n        chain_type_kwargs={\n            \"prompt\": PromptTemplate(\n                template=\"Answer the question based on the context:\\n\\n{context}\\n\\nQuestion: {question}\",\n                input_variables=[\"context\", \"question\"]\n            )\n        }\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:51:35.606498Z","iopub.execute_input":"2025-07-10T12:51:35.60675Z","iopub.status.idle":"2025-07-10T12:51:35.621173Z","shell.execute_reply.started":"2025-07-10T12:51:35.606735Z","shell.execute_reply":"2025-07-10T12:51:35.620546Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"class ExtendedMistralLLM(MistralLLM):\n    def query(self, vectordb: Chroma, query: str, k: int = 5) -> str:\n        if not self.llm:\n            return \"LLM not initialized\"\n            \n        qa_chain = create_qa_chain(self.llm, vectordb, k)\n        try:\n            result = qa_chain.invoke({\"query\": query})\n            return result[\"result\"]\n        except Exception as e:\n            return f\"Error processing query: {str(e)}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:52:31.589707Z","iopub.execute_input":"2025-07-10T12:52:31.590513Z","iopub.status.idle":"2025-07-10T12:52:31.595357Z","shell.execute_reply.started":"2025-07-10T12:52:31.590489Z","shell.execute_reply":"2025-07-10T12:52:31.594684Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def main():\n    processor = DocumentProcessor()\n    vs_manager = VectorStoreManager()\n    llm = ExtendedMistralLLM()\n\n    document_paths = [\n        \"/kaggle/input/requireddoc/Recommendation.pdf\",\n        \"/kaggle/input/daafile/End.pdf\",\n        \"/kaggle/input/womensafetycasestudy/Women Safety_v3.pdf\",\n        \"/kaggle/input/multipledata/Programooad.docx\",\n        \"/kaggle/input/multipledata/A.docx\",\n        \"/kaggle/input/multipledata/r.txt\"\n    ][:Config.MAX_DOCUMENTS]\n\n    all_docs = []\n    for path in document_paths:\n        if os.path.getsize(path) > Config.MAX_DOC_SIZE:\n            print(f\"Skipping {path} - exceeds size limit\")\n            continue\n            \n        docs = processor.process_file(path)\n        if docs:\n            all_docs.extend(docs)\n            print(f\"Processed {path}\")\n        else:\n            print(f\"Failed to process {path}\")\n\n    if all_docs:\n        vectordb = vs_manager.create_store(all_docs)\n        if vectordb:\n            print(\"Vector store created\")\n            while True:\n                query = input(\"\\nEnter question (or 'exit'): \")\n                if query.lower() == 'exit':\n                    break\n                print(llm.query(vectordb, query))\n        else:\n            print(\"Failed to create vector store\")\n    else:\n        print(\"No documents processed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:40:15.349295Z","iopub.execute_input":"2025-07-10T13:40:15.349988Z","iopub.status.idle":"2025-07-10T13:40:15.355984Z","shell.execute_reply.started":"2025-07-10T13:40:15.34996Z","shell.execute_reply":"2025-07-10T13:40:15.355184Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\nfrom IPython.display import clear_output\n\ndef clear_memory():\n    \"\"\"Comprehensive memory cleanup for Kaggle notebooks\"\"\"\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            print(\"Cleared CUDA cache\")\n        gc.collect()\n        print(\"Ran garbage collection\")\n        clear_output(wait=True)\n        if torch.cuda.is_available():\n            print(f\"Current GPU memory usage: {torch.cuda.memory_allocated()/1024**2:.2f}MB / {torch.cuda.memory_reserved()/1024**2:.2f}MB\")\n        else:\n            print(\"Memory cleared (CPU-only mode)\")\n            \n    except Exception as e:\n        print(f\"Error clearing memory: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:52:47.918347Z","iopub.execute_input":"2025-07-10T12:52:47.918667Z","iopub.status.idle":"2025-07-10T12:52:47.924935Z","shell.execute_reply.started":"2025-07-10T12:52:47.91864Z","shell.execute_reply":"2025-07-10T12:52:47.924079Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    clear_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:52:52.590128Z","iopub.execute_input":"2025-07-10T12:52:52.590665Z","iopub.status.idle":"2025-07-10T12:52:53.019037Z","shell.execute_reply.started":"2025-07-10T12:52:52.590642Z","shell.execute_reply":"2025-07-10T12:52:53.018255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    import warnings\n    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.module\")\n    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T12:52:56.952953Z","iopub.execute_input":"2025-07-10T12:52:56.953249Z","iopub.status.idle":"2025-07-10T13:02:39.869501Z","shell.execute_reply.started":"2025-07-10T12:52:56.953227Z","shell.execute_reply":"2025-07-10T13:02:39.868679Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    clear_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:02:46.035043Z","iopub.execute_input":"2025-07-10T13:02:46.035673Z","iopub.status.idle":"2025-07-10T13:02:46.552731Z","shell.execute_reply.started":"2025-07-10T13:02:46.035649Z","shell.execute_reply":"2025-07-10T13:02:46.552133Z"}},"outputs":[],"execution_count":null}]}