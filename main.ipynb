{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10409147,"sourceType":"datasetVersion","datasetId":6450551},{"sourceId":10549491,"sourceType":"datasetVersion","datasetId":6527299},{"sourceId":12431403,"sourceType":"datasetVersion","datasetId":7841431},{"sourceId":12431421,"sourceType":"datasetVersion","datasetId":7841445},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":3900,"modelId":1902}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adityabhaskar12/rag-originbluy?scriptVersionId=249940418\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"**Package Installation**\nInstalls all necessary Python dependencies for the RAG system:\n- **OCR/Image Processing**: opencv-python, pdf2image, Pillow, EasyOCR\n- **Document Parsing**: PyMuPDF (PDFs), python-docx (Word), mammoth\n- **ML/NLP**: transformers, sentence-transformers, torch\n- **Vector DB**: chromadb\n- **Utils**: numpy, langchain-community","metadata":{}},{"cell_type":"code","source":"!pip install opencv-python\n!pip install PyMuPDF\n!pip install transformers\n!pip install torch\n!pip install numpy\n!pip install sentence-transformers\n!pip install pdf2image\n!pip install python-docx\n!pip install mammoth\n!pip install chromadb\n!pip install Pillow\n!pip install git+https://github.com/JaidedAI/EasyOCR.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:08:09.216226Z","iopub.execute_input":"2025-07-11T07:08:09.216473Z","iopub.status.idle":"2025-07-11T07:11:13.01399Z","shell.execute_reply.started":"2025-07-11T07:08:09.216453Z","shell.execute_reply":"2025-07-11T07:11:13.013304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## System Configuration\nInstalls essential system libraries:\n- `poppler-utils`: PDF rendering\n- `tesseract-ocr`: OCR engine\n- `libgl1`: OpenGL support for GUI-less environments","metadata":{}},{"cell_type":"code","source":"!sudo apt update\n!sudo apt install -y poppler-utils tesseract-ocr libgl1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:11:47.463374Z","iopub.execute_input":"2025-07-11T07:11:47.463628Z","iopub.status.idle":"2025-07-11T07:12:00.544862Z","shell.execute_reply.started":"2025-07-11T07:11:47.463606Z","shell.execute_reply":"2025-07-11T07:12:00.544167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ”„ LangChain Update\nUpgrades `langchain-community` to ensure compatibility with the latest document processing features.","metadata":{}},{"cell_type":"code","source":"!pip install -U langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:12:06.06345Z","iopub.execute_input":"2025-07-11T07:12:06.064126Z","iopub.status.idle":"2025-07-11T07:12:12.033161Z","shell.execute_reply.started":"2025-07-11T07:12:06.064085Z","shell.execute_reply":"2025-07-11T07:12:12.032367Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ¤— HuggingFace Setup\nInstalls LangChain-HuggingFace integration for:\n- Seamless transformer model usage\n- Pipeline management\n- Optimized LLM inference","metadata":{}},{"cell_type":"code","source":"!pip install -U langchain-huggingface transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:12:13.285171Z","iopub.execute_input":"2025-07-11T07:12:13.286059Z","iopub.status.idle":"2025-07-11T07:12:26.799272Z","shell.execute_reply.started":"2025-07-11T07:12:13.286028Z","shell.execute_reply":"2025-07-11T07:12:26.798492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Import Dependencies\nKey imports organized by functionality:\n\n### **Document Processing**\n- `fitz` (PyMuPDF), `Document` (docx), `pdf2image`\n- `easyocr` for OCR\n\n### **NLP/ML**\n- HuggingFace `transformers`, `pipeline`\n- `TableTransformerForObjectDetection` (table extraction)\n\n### **Vector DB**\n- `Chroma` vector store\n- `HuggingFaceEmbeddings`\n\n### **Utils**\n- `RecursiveCharacterTextSplitter` for chunking\n- `PromptTemplate` for LLM instructions","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport cv2\nimport fitz \nfrom docx import Document\nfrom typing import List, Dict, Any, Optional\nfrom pdf2image import convert_from_path\nimport easyocr\nimport torch\nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document as LangchainDocument\nfrom langchain.prompts import PromptTemplate\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    pipeline,\n    TableTransformerForObjectDetection,\n    DetrImageProcessor\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:12:27.223935Z","iopub.execute_input":"2025-07-11T07:12:27.22464Z","iopub.status.idle":"2025-07-11T07:13:01.310619Z","shell.execute_reply.started":"2025-07-11T07:12:27.224611Z","shell.execute_reply":"2025-07-11T07:13:01.310058Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## System Configuration\nCentralized settings via `Config` class:\n\n    CHROMA_DIR = \"chroma_db\"  # Vector storage\n    \n    MISTRAL_PATH = \"/kaggle/input/mistral/...\"  # LLM path  \n    \n    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  \n    \n    SUPPORTED_EXTS = {'.pdf', '.docx', '.txt'}  # File types\n    \n    MAX_DOC_SIZE = 100MB  # Total file size limit\n    \n    DEVICE = \"cuda\" if available else \"cpu\"  # Hardware acceleration \n                                             # if nvidia gpu is provided uses that else uses cpu","metadata":{}},{"cell_type":"code","source":"class Config:\n    CHROMA_DIR = \"chroma_db\"\n    MISTRAL_PATH = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n    SUPPORTED_EXTS = {'.pdf', '.docx', '.txt'}\n    MAX_DOC_SIZE = 100 * 1024 * 1024  \n    MAX_DOCUMENTS = 10\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:13:09.406191Z","iopub.execute_input":"2025-07-11T07:13:09.406471Z","iopub.status.idle":"2025-07-11T07:13:09.411156Z","shell.execute_reply.started":"2025-07-11T07:13:09.40645Z","shell.execute_reply":"2025-07-11T07:13:09.410519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Document Processor\nAdvanced multi-format parser with:\n\n### Key Features:\n1. **PDF Processing**\n   - OCR with EasyOCR\n   - Table detection (Table Transformer)\n   - Fallback to PyMuPDF text extraction\n\n2. **DOCX Support**\n   - Paragraph extraction\n   - Table content preservation\n\n3. **TXT FILE Support**","metadata":{"execution":{"iopub.status.busy":"2025-07-10T16:20:52.925276Z","iopub.execute_input":"2025-07-10T16:20:52.925565Z","iopub.status.idle":"2025-07-10T16:20:52.936465Z","shell.execute_reply.started":"2025-07-10T16:20:52.92554Z","shell.execute_reply":"2025-07-10T16:20:52.934965Z"}}},{"cell_type":"code","source":"class DocumentProcessor:\n    def __init__(self):\n        try:\n            self.ocr_reader = easyocr.Reader(['en'])\n            self.table_processor = DetrImageProcessor.from_pretrained(\n               \"microsoft/table-transformer-detection\",\n                size={\"longest_edge\": 1000}, \n                use_fast=True\n            )\n            self.table_detector = TableTransformerForObjectDetection.from_pretrained(\n                \"microsoft/table-transformer-detection\"\n            ).to(Config.DEVICE)\n        except Exception as e:\n            print(f\"Initialization error: {str(e)}\")\n            raise\n\n    def process_file(self, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Main method to process any supported file type\"\"\"\n        ext = os.path.splitext(file_path)[1].lower()\n        try:\n            if ext == '.pdf':\n                return self._process_pdf(file_path)\n            elif ext == '.docx':\n                return self._process_docx(file_path)\n            elif ext == '.txt':\n                return self._process_txt(file_path)\n        except Exception as e:\n            print(f\"Error processing {file_path}: {str(e)}\")\n            return []\n\n    def _process_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Process PDF with OCR and table detection, with robust fallback\"\"\"\n        try:\n            advanced_result = self._process_pdf_with_ocr_and_tables(pdf_path)\n            if advanced_result:\n                return advanced_result\n        except Exception as e:\n            print(f\"Advanced processing failed: {str(e)}\")\n        try:\n            doc = fitz.open(pdf_path)\n            return [{\n                \"page_number\": i + 1,\n                \"content\": [{\"type\": \"text\", \"content\": page.get_text()}],\n                \"source\": os.path.basename(pdf_path)\n            } for i, page in enumerate(doc)]\n        except Exception as e:\n            print(f\"Simple PDF processing failed: {str(e)}\")\n            return []\n\n    def _process_pdf_with_ocr_and_tables(self, pdf_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Advanced PDF processing with OCR and table detection\"\"\"\n        try:\n            images = convert_from_path(pdf_path)\n            doc = fitz.open(pdf_path)\n            extracted_data = []\n\n            for page_num, (image, page) in enumerate(zip(images, doc), 1):\n                page_content = []\n                cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n\n                # TABLE DETECTION (only if GPU available)\n                table_boxes = []\n                if torch.cuda.is_available():\n                    try:\n                        inputs = self.table_processor(images=image, return_tensors=\"pt\").to(Config.DEVICE)\n                        with torch.no_grad():\n                            outputs = self.table_detector(**inputs)\n\n                        target_sizes = torch.tensor([image.size[::-1]])\n                        results = self.table_processor.post_process_object_detection(\n                            outputs, threshold=0.9, target_sizes=target_sizes\n                        )[0]\n\n                        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n                            if score > 0.9 and label == self.table_detector.config.id2label[1]:\n                                box = [int(i) for i in box.tolist()]\n                                table_boxes.append((box[1], box[0], box[3], box[2]))\n                    except Exception as e:\n                        print(f\"Table detection failed, continuing without tables: {str(e)}\")\n                try:\n                    if table_boxes:\n                        mask = np.ones(cv_image.shape[:2], dtype=np.uint8) * 255\n                        for y0, x0, y1, x1 in table_boxes:\n                            cv2.rectangle(mask, (x0, y0), (x1, y1), 0, -1)\n                        masked_image = cv2.bitwise_and(cv_image, cv_image, mask=mask)\n                    else:\n                        masked_image = cv_image\n                    ocr_results = self.ocr_reader.readtext(masked_image, paragraph=True)\n                    for result in ocr_results:\n                        try:\n                            if len(result) >= 2:\n                                text = result[1]\n                                bbox = result[0]\n                                page_content.append({\n                                    \"type\": \"text\",\n                                    \"content\": text,\n                                    \"position\": [int(coord) for point in bbox for coord in point] if bbox else []\n                                })\n                        except Exception as e:\n                            print(f\"Error processing OCR result: {str(e)}\")\n\n                except Exception as e:\n                    print(f\"OCR failed for page {page_num}, falling back to simple text: {str(e)}\")\n                    page_content.append({\n                        \"type\": \"text\",\n                        \"content\": page.get_text(),\n                        \"position\": []\n                    })\n                for y0, x0, y1, x1 in table_boxes:\n                    try:\n                        table_img = cv_image[y0:y1, x0:x1]\n                        table_results = self.ocr_reader.readtext(table_img)\n                        table_content = [res[1] for res in table_results if len(res) >= 2]\n\n                        page_content.append({\n                            \"type\": \"table\",\n                            \"content\": table_content,\n                            \"position\": [x0, y0, x1, y1]\n                        })\n                    except Exception as e:\n                        print(f\"Table processing failed: {str(e)}\")\n\n                extracted_data.append({\n                    \"page_number\": page_num,\n                    \"content\": page_content,\n                    \"source\": os.path.basename(pdf_path)\n                })\n\n            return extracted_data\n\n        except Exception as e:\n            raise Exception(f\"PDF processing failed: {str(e)}\")\n\n    def _process_docx(self, docx_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Process DOCX files with table support\"\"\"\n        try:\n            doc = Document(docx_path)\n            content = []\n            for para in doc.paragraphs:\n                if para.text.strip():\n                    content.append({\n                        \"type\": \"text\",\n                        \"content\": para.text\n                    })\n            for table in doc.tables:\n                table_data = []\n                for row in table.rows:\n                    row_data = [cell.text for cell in row.cells]\n                    table_data.append(row_data)\n                content.append({\n                    \"type\": \"table\",\n                    \"content\": table_data\n                })\n\n            return [{\n                \"page_number\": 1,\n                \"content\": content,\n                \"source\": os.path.basename(docx_path)\n            }]\n        except Exception as e:\n            print(f\"DOCX processing error: {str(e)}\")\n            return []\n\n    def _process_txt(self, txt_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Process plain text files\"\"\"\n        try:\n            with open(txt_path, 'r', encoding='utf-8') as f:\n                return [{\n                    \"page_number\": 1,\n                    \"content\": [{\"type\": \"text\", \"content\": f.read()}],\n                    \"source\": os.path.basename(txt_path)\n                }]\n        except Exception as e:\n            print(f\"TXT processing error: {str(e)}\")\n            return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:13:13.968327Z","iopub.execute_input":"2025-07-11T07:13:13.968601Z","iopub.status.idle":"2025-07-11T07:13:13.988314Z","shell.execute_reply.started":"2025-07-11T07:13:13.968581Z","shell.execute_reply":"2025-07-11T07:13:13.987776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Vector Database Setup\nCreates search-optimized document storage:\n","metadata":{}},{"cell_type":"code","source":"class VectorStoreManager:\n    def __init__(self):\n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=Config.EMBEDDING_MODEL,\n            model_kwargs={'device': Config.DEVICE}\n        )\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200\n        )\n\n    def create_store(self, documents: List[Dict]) -> Optional[Chroma]:\n        if not documents:\n            return None\n\n        lc_docs = []\n        for doc in documents:\n            for content in doc['content']:\n                if content['type'] == 'text':\n                    for chunk in self.text_splitter.split_text(content['content']):\n                        lc_docs.append(LangchainDocument(\n                            page_content=chunk,\n                            metadata={} \n                        ))\n                elif content['type'] == 'table':\n                    table_text = \" | \".join([\" | \".join(row) for row in content['content']])\n                    lc_docs.append(LangchainDocument(\n                        page_content=f\"TABLE: {table_text}\",\n                        metadata={}\n                    ))\n\n        return Chroma.from_documents(\n            documents=lc_docs,\n            embedding=self.embedding_model,\n            persist_directory=Config.CHROMA_DIR\n        ) if lc_docs else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:13:20.865703Z","iopub.execute_input":"2025-07-11T07:13:20.866233Z","iopub.status.idle":"2025-07-11T07:13:20.872265Z","shell.execute_reply.started":"2025-07-11T07:13:20.866207Z","shell.execute_reply":"2025-07-11T07:13:20.871637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LLM Initialization\n```markdown\nMistral-7B LLM Wrapper","metadata":{}},{"cell_type":"code","source":"class MistralLLM:\n    def __init__(self):\n        try:\n            from langchain_community.llms import HuggingFacePipeline  # Correct import\n            \n            self.tokenizer = AutoTokenizer.from_pretrained(Config.MISTRAL_PATH)\n            self.model = AutoModelForCausalLM.from_pretrained(\n                Config.MISTRAL_PATH,\n                torch_dtype=torch.float16,\n                device_map=\"auto\"\n            )\n            self.pipe = pipeline(\n                \"text-generation\",\n                model=self.model,\n                tokenizer=self.tokenizer,\n                max_new_tokens=512,\n                temperature=0.1,\n                device_map=\"auto\"\n            )\n            self.llm = HuggingFacePipeline(pipeline=self.pipe)\n        except Exception as e:\n            print(f\"LLM initialization failed: {str(e)}\")\n            self.llm = None\n\n    def query(self, vectordb: Chroma, query: str, k: int = 3) -> str:\n        if not self.llm:\n            return \"LLM not initialized\"\n            \n        retriever = vectordb.as_retriever(search_kwargs={\"k\": k})\n        qa = RetrievalQA.from_chain_type(\n            llm=self.llm,\n            chain_type=\"stuff\",\n            retriever=retriever,\n            return_source_documents=True\n        )\n        result = qa({\"query\": query})\n        return f\"Answer: {result['result']}\\nSources: {[d.metadata['source'] for d in result['source_documents']]}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:13:25.578498Z","iopub.execute_input":"2025-07-11T07:13:25.578775Z","iopub.status.idle":"2025-07-11T07:13:25.585156Z","shell.execute_reply.started":"2025-07-11T07:13:25.578752Z","shell.execute_reply":"2025-07-11T07:13:25.584526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prompt Template\nStructured prompt enforcing:\n- Source synthesis\n- Conflict resolution\n- Bullet-point formatting","metadata":{}},{"cell_type":"code","source":"RAG_PROMPT_TEMPLATE = \"\"\"As an expert document analyst with access to multiple sources, your task is to provide the most accurate, well-structured answer to the user's question:\n\n1. Context Analysis:\n{context}\n\n2. Question:\n{question}\n\n3. Answer Generation Rules:\n- Synthesize information from all relevant context\n- Acknowledge when information is incomplete\n- Break down complex answers into bullet points when helpful\n- Highlight key statistics, names, and dates\n- If conflicting information exists, present both sides with sources\n- For technical queries, provide detailed explanations\n- For summary requests, include all major points\n\n4. Required Answer Format:\n[Start of Answer]\n### Comprehensive Response:\n[Your synthesized answer here]\nonly include relevant sources\n[End of Answer]\"\"\"\nrag_prompt = PromptTemplate(\n    template=RAG_PROMPT_TEMPLATE,\n    input_variables=[\"context\", \"question\"]\n)\ndef create_qa_chain(llm, vectordb, k=3):\n    return RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=vectordb.as_retriever(search_kwargs={\"k\": k}),\n        return_source_documents=True,  \n        chain_type_kwargs={\n            \"prompt\": rag_prompt, \n            \"document_prompt\": PromptTemplate(\n                input_variables=[\"page_content\"],\n                template=\"{page_content}\" \n            )\n        }\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:13:31.533665Z","iopub.execute_input":"2025-07-11T07:13:31.534321Z","iopub.status.idle":"2025-07-11T07:13:31.538859Z","shell.execute_reply.started":"2025-07-11T07:13:31.534297Z","shell.execute_reply":"2025-07-11T07:13:31.538353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Enhanced Query Handling\nExtends base LLM with:\n- Custom retrieval-augmented generation\n- Error handling\n- Simplified response format","metadata":{}},{"cell_type":"code","source":"class ExtendedMistralLLM(MistralLLM):\n    def query(self, vectordb: Chroma, query: str, k: int = 3) -> str:\n        if not self.llm:\n            return \"LLM not initialized\"\n            \n        qa_chain = create_qa_chain(self.llm, vectordb, k)\n        try:\n            result = qa_chain.invoke({\"query\": query})\n            return result[\"result\"]\n        except Exception as e:\n            return f\"Error processing query: {str(e)}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:13:36.402211Z","iopub.execute_input":"2025-07-11T07:13:36.402807Z","iopub.status.idle":"2025-07-11T07:13:36.407053Z","shell.execute_reply.started":"2025-07-11T07:13:36.402784Z","shell.execute_reply":"2025-07-11T07:13:36.406502Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Execution Pipeline\nEnd-to-end document processing flow:\n\n1. **Input:** List of document paths\n2. **Processing:**\n   - Size validation\n   - Parallel parsing\n3. **Query Loop:**\n   - Interactive question answering","metadata":{}},{"cell_type":"code","source":"def main():\n    processor = DocumentProcessor()\n    vs_manager = VectorStoreManager()\n    llm = ExtendedMistralLLM()\n\n    document_paths = [\n        \"/kaggle/input/daafile/End Sem DAA.pdf\",\n        \"/kaggle/input/txtfile/SampleTextFile_1000kb.txt\",\n        \"/kaggle/input/womensafetycasestudy/Women Safety_v3.pdf\",\n        \"/kaggle/input/wordfile/uhvprojectcopy.docx\",\n    ][:Config.MAX_DOCUMENTS]\n\n    all_docs = []\n    for path in document_paths:\n        if os.path.getsize(path) > Config.MAX_DOC_SIZE:\n            print(f\"Skipping {path} - exceeds size limit\")\n            continue\n            \n        docs = processor.process_file(path)\n        if docs:\n            all_docs.extend(docs)\n            print(f\"Processed {path}\")\n        else:\n            print(f\"Failed to process {path}\")\n\n    if all_docs:\n        vectordb = vs_manager.create_store(all_docs)\n        if vectordb:\n            print(\"Vector store created\")\n            while True:\n                query = input(\"\\nEnter question (or 'exit'): \")\n                if query.lower() == 'exit':\n                    break\n                print(llm.query(vectordb, query))\n        else:\n            print(\"Failed to create vector store\")\n    else:\n        print(\"No documents processed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:13:41.313666Z","iopub.execute_input":"2025-07-11T07:13:41.314378Z","iopub.status.idle":"2025-07-11T07:13:41.319911Z","shell.execute_reply.started":"2025-07-11T07:13:41.31435Z","shell.execute_reply":"2025-07-11T07:13:41.319162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Memory Optimization\nProactive resource cleanup","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\nfrom IPython.display import clear_output\n\ndef clear_memory():\n    \"\"\"Comprehensive memory cleanup for Kaggle notebooks\"\"\"\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            print(\"Cleared CUDA cache\")\n        gc.collect()\n        print(\"Ran garbage collection\")\n        clear_output(wait=True)\n        if torch.cuda.is_available():\n            print(f\"Current GPU memory usage: {torch.cuda.memory_allocated()/1024**2:.2f}MB / {torch.cuda.memory_reserved()/1024**2:.2f}MB\")\n        else:\n            print(\"Memory cleared (CPU-only mode)\")\n            \n    except Exception as e:\n        print(f\"Error clearing memory: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:13:46.362694Z","iopub.execute_input":"2025-07-11T07:13:46.362975Z","iopub.status.idle":"2025-07-11T07:13:46.368199Z","shell.execute_reply.started":"2025-07-11T07:13:46.362955Z","shell.execute_reply":"2025-07-11T07:13:46.367184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Model loading\n- CUDA operations","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    import warnings\n    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.module\")\n    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:13:56.013615Z","iopub.execute_input":"2025-07-11T07:13:56.014177Z","iopub.status.idle":"2025-07-11T07:27:59.871061Z","shell.execute_reply.started":"2025-07-11T07:13:56.014147Z","shell.execute_reply":"2025-07-11T07:27:59.870152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Final Cleanup after use","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    clear_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:28:05.968561Z","iopub.execute_input":"2025-07-11T07:28:05.968861Z","iopub.status.idle":"2025-07-11T07:28:06.491239Z","shell.execute_reply.started":"2025-07-11T07:28:05.968838Z","shell.execute_reply":"2025-07-11T07:28:06.490689Z"}},"outputs":[],"execution_count":null}]}